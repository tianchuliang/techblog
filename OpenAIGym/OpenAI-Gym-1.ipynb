{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, math\n",
    "import numpy as np\n",
    "\n",
    "import Box2D\n",
    "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rocket trajectory optimization is a classic topic in Optimal Control.\n",
    "#\n",
    "# According to Pontryagin's maximum principle it's optimal to fire engine full throttle or\n",
    "# turn it off. That's the reason this environment is OK to have discreet actions (engine on or off).\n",
    "#\n",
    "# Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector.\n",
    "# Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.\n",
    "# If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or\n",
    "# comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main\n",
    "# engine is -0.3 points each frame. Solved is 200 points.\n",
    "#\n",
    "# Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "# on its first attempt. Please see source code for details.\n",
    "#\n",
    "# Too see heuristic landing, run:\n",
    "#\n",
    "# python gym/envs/box2d/lunar_lander.py\n",
    "#\n",
    "# To play yourself, run:\n",
    "#\n",
    "# python examples/agents/keyboard_agent.py LunarLander-v0\n",
    "#\n",
    "# Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FPS    = 30\n",
    "SCALE  = 30.0   # affects how fast-paced the game is, forces should be adjusted as well\n",
    "\n",
    "MAIN_ENGINE_POWER  = 13.0\n",
    "SIDE_ENGINE_POWER  =  0.6\n",
    "\n",
    "INITIAL_RANDOM = 500.0   # Set 1500 to make game harder\n",
    "\n",
    "LANDER_POLY =[\n",
    "    (-14,+17), (-17,0), (-17,-10),\n",
    "    (+17,-10), (+17,0), (+14,+17)\n",
    "    ]\n",
    "LEG_AWAY = 20\n",
    "LEG_DOWN = 18\n",
    "LEG_W, LEG_H = 2, 8\n",
    "LEG_SPRING_TORQUE = 50\n",
    "\n",
    "SIDE_ENGINE_HEIGHT = 14.0\n",
    "SIDE_ENGINE_AWAY   = 12.0\n",
    "\n",
    "VIEWPORT_W = 600\n",
    "VIEWPORT_H = 400\n",
    "\n",
    "class ContactDetector(contactListener):\n",
    "    def __init__(self, env):\n",
    "        contactListener.__init__(self)\n",
    "        self.env = env\n",
    "    def BeginContact(self, contact):\n",
    "        if self.env.lander==contact.fixtureA.body or self.env.lander==contact.fixtureB.body:\n",
    "            self.env.game_over = True\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = True\n",
    "    def EndContact(self, contact):\n",
    "        for i in range(2):\n",
    "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
    "                self.env.legs[i].ground_contact = False\n",
    "\n",
    "class LunarLander(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second' : FPS\n",
    "    }\n",
    "\n",
    "    continuous = False\n",
    "\n",
    "    def __init__(self):\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "\n",
    "        self.world = Box2D.b2World()\n",
    "        self.moon = None\n",
    "        self.lander = None\n",
    "        self.particles = []\n",
    "\n",
    "        self.prev_reward = None\n",
    "\n",
    "        high = np.array([np.inf]*8)  # useful range is -1 .. +1, but spikes can be higher\n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "        if self.continuous:\n",
    "            # Action is two floats [main engine, left-right engines].\n",
    "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
    "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
    "            self.action_space = spaces.Box(-1, +1, (2,))\n",
    "        else:\n",
    "            # Nop, fire left engine, main engine, right engine\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _destroy(self):\n",
    "        if not self.moon: return\n",
    "        self.world.contactListener = None\n",
    "        self._clean_particles(True)\n",
    "        self.world.DestroyBody(self.moon)\n",
    "        self.moon = None\n",
    "        self.world.DestroyBody(self.lander)\n",
    "        self.lander = None\n",
    "        self.world.DestroyBody(self.legs[0])\n",
    "        self.world.DestroyBody(self.legs[1])\n",
    "\n",
    "    def reset(self):\n",
    "        self._destroy()\n",
    "        self.world.contactListener_keepref = ContactDetector(self)\n",
    "        self.world.contactListener = self.world.contactListener_keepref\n",
    "        self.game_over = False\n",
    "        self.prev_shaping = None\n",
    "\n",
    "        W = VIEWPORT_W/SCALE\n",
    "        H = VIEWPORT_H/SCALE\n",
    "\n",
    "        # terrain\n",
    "        CHUNKS = 11\n",
    "        height = self.np_random.uniform(0, H/2, size=(CHUNKS+1,) )\n",
    "        chunk_x  = [W/(CHUNKS-1)*i for i in range(CHUNKS)]\n",
    "        self.helipad_x1 = chunk_x[CHUNKS//2-1]\n",
    "        self.helipad_x2 = chunk_x[CHUNKS//2+1]\n",
    "        self.helipad_y  = H/4\n",
    "        height[CHUNKS//2-2] = self.helipad_y\n",
    "        height[CHUNKS//2-1] = self.helipad_y\n",
    "        height[CHUNKS//2+0] = self.helipad_y\n",
    "        height[CHUNKS//2+1] = self.helipad_y\n",
    "        height[CHUNKS//2+2] = self.helipad_y\n",
    "        smooth_y = [0.33*(height[i-1] + height[i+0] + height[i+1]) for i in range(CHUNKS)]\n",
    "\n",
    "        self.moon = self.world.CreateStaticBody( shapes=edgeShape(vertices=[(0, 0), (W, 0)]) )\n",
    "        self.sky_polys = []\n",
    "        for i in range(CHUNKS-1):\n",
    "            p1 = (chunk_x[i],   smooth_y[i])\n",
    "            p2 = (chunk_x[i+1], smooth_y[i+1])\n",
    "            self.moon.CreateEdgeFixture(\n",
    "                vertices=[p1,p2],\n",
    "                density=0,\n",
    "                friction=0.1)\n",
    "            self.sky_polys.append( [p1, p2, (p2[0],H), (p1[0],H)] )\n",
    "\n",
    "        self.moon.color1 = (0.0,0.0,0.0)\n",
    "        self.moon.color2 = (0.0,0.0,0.0)\n",
    "\n",
    "        initial_y = VIEWPORT_H/SCALE\n",
    "        self.lander = self.world.CreateDynamicBody(\n",
    "            position = (VIEWPORT_W/SCALE/2, initial_y),\n",
    "            angle=0.0,\n",
    "            fixtures = fixtureDef(\n",
    "                shape=polygonShape(vertices=[ (x/SCALE,y/SCALE) for x,y in LANDER_POLY ]),\n",
    "                density=5.0,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0010,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.0) # 0.99 bouncy\n",
    "                )\n",
    "        self.lander.color1 = (0.5,0.4,0.9)\n",
    "        self.lander.color2 = (0.3,0.3,0.5)\n",
    "        self.lander.ApplyForceToCenter( (\n",
    "            self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
    "            self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM)\n",
    "            ), True)\n",
    "\n",
    "        self.legs = []\n",
    "        for i in [-1,+1]:\n",
    "            leg = self.world.CreateDynamicBody(\n",
    "                position = (VIEWPORT_W/SCALE/2 - i*LEG_AWAY/SCALE, initial_y),\n",
    "                angle = (i*0.05),\n",
    "                fixtures = fixtureDef(\n",
    "                    shape=polygonShape(box=(LEG_W/SCALE, LEG_H/SCALE)),\n",
    "                    density=1.0,\n",
    "                    restitution=0.0,\n",
    "                    categoryBits=0x0020,\n",
    "                    maskBits=0x001)\n",
    "                )\n",
    "            leg.ground_contact = False\n",
    "            leg.color1 = (0.5,0.4,0.9)\n",
    "            leg.color2 = (0.3,0.3,0.5)\n",
    "            rjd = revoluteJointDef(\n",
    "                bodyA=self.lander,\n",
    "                bodyB=leg,\n",
    "                localAnchorA=(0, 0),\n",
    "                localAnchorB=(i*LEG_AWAY/SCALE, LEG_DOWN/SCALE),\n",
    "                enableMotor=True,\n",
    "                enableLimit=True,\n",
    "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
    "                motorSpeed=+0.3*i  # low enough not to jump back into the sky\n",
    "                )\n",
    "            if i==-1:\n",
    "                rjd.lowerAngle = +0.9 - 0.5  # Yes, the most esoteric numbers here, angles legs have freedom to travel within\n",
    "                rjd.upperAngle = +0.9\n",
    "            else:\n",
    "                rjd.lowerAngle = -0.9\n",
    "                rjd.upperAngle = -0.9 + 0.5\n",
    "            leg.joint = self.world.CreateJoint(rjd)\n",
    "            self.legs.append(leg)\n",
    "\n",
    "        self.drawlist = [self.lander] + self.legs\n",
    "\n",
    "        return self.step(np.array([0,0]) if self.continuous else 0)[0]\n",
    "\n",
    "    def _create_particle(self, mass, x, y, ttl):\n",
    "        p = self.world.CreateDynamicBody(\n",
    "            position = (x,y),\n",
    "            angle=0.0,\n",
    "            fixtures = fixtureDef(\n",
    "                shape=circleShape(radius=2/SCALE, pos=(0,0)),\n",
    "                density=mass,\n",
    "                friction=0.1,\n",
    "                categoryBits=0x0100,\n",
    "                maskBits=0x001,  # collide only with ground\n",
    "                restitution=0.3)\n",
    "                )\n",
    "        p.ttl = ttl\n",
    "        self.particles.append(p)\n",
    "        self._clean_particles(False)\n",
    "        return p\n",
    "\n",
    "    def _clean_particles(self, all):\n",
    "        while self.particles and (all or self.particles[0].ttl<0):\n",
    "            self.world.DestroyBody(self.particles.pop(0))\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid \" % (action,type(action))\n",
    "\n",
    "        # Engines\n",
    "        tip  = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
    "        side = (-tip[1], tip[0]);\n",
    "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
    "\n",
    "        m_power = 0.0\n",
    "        if (self.continuous and action[0] > 0.0) or (not self.continuous and action==2):\n",
    "            # Main engine\n",
    "            if self.continuous:\n",
    "                m_power = (np.clip(action[0], 0.0,1.0) + 1.0)*0.5   # 0.5..1.0\n",
    "                assert m_power>=0.5 and m_power <= 1.0\n",
    "            else:\n",
    "                m_power = 1.0\n",
    "            ox =  tip[0]*(4/SCALE + 2*dispersion[0]) + side[0]*dispersion[1]   # 4 is move a bit downwards, +-2 for randomness\n",
    "            oy = -tip[1]*(4/SCALE + 2*dispersion[0]) - side[1]*dispersion[1]\n",
    "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
    "            p = self._create_particle(3.5, impulse_pos[0], impulse_pos[1], m_power)    # particles are just a decoration, 3.5 is here to make particle speed adequate\n",
    "            p.ApplyLinearImpulse(           ( ox*MAIN_ENGINE_POWER*m_power,  oy*MAIN_ENGINE_POWER*m_power), impulse_pos, True)\n",
    "            self.lander.ApplyLinearImpulse( (-ox*MAIN_ENGINE_POWER*m_power, -oy*MAIN_ENGINE_POWER*m_power), impulse_pos, True)\n",
    "\n",
    "        s_power = 0.0\n",
    "        if (self.continuous and np.abs(action[1]) > 0.5) or (not self.continuous and action in [1,3]):\n",
    "            # Orientation engines\n",
    "            if self.continuous:\n",
    "                direction = np.sign(action[1])\n",
    "                s_power = np.clip(np.abs(action[1]), 0.5,1.0)\n",
    "                assert s_power>=0.5 and s_power <= 1.0\n",
    "            else:\n",
    "                direction = action-2\n",
    "                s_power = 1.0\n",
    "            ox =  tip[0]*dispersion[0] + side[0]*(3*dispersion[1]+direction*SIDE_ENGINE_AWAY/SCALE)\n",
    "            oy = -tip[1]*dispersion[0] - side[1]*(3*dispersion[1]+direction*SIDE_ENGINE_AWAY/SCALE)\n",
    "            impulse_pos = (self.lander.position[0] + ox - tip[0]*17/SCALE, self.lander.position[1] + oy + tip[1]*SIDE_ENGINE_HEIGHT/SCALE)\n",
    "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
    "            p.ApplyLinearImpulse(           ( ox*SIDE_ENGINE_POWER*s_power,  oy*SIDE_ENGINE_POWER*s_power), impulse_pos, True)\n",
    "            self.lander.ApplyLinearImpulse( (-ox*SIDE_ENGINE_POWER*s_power, -oy*SIDE_ENGINE_POWER*s_power), impulse_pos, True)\n",
    "\n",
    "        self.world.Step(1.0/FPS, 6*30, 2*30)\n",
    "\n",
    "        pos = self.lander.position\n",
    "        vel = self.lander.linearVelocity\n",
    "        state = [\n",
    "            (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),\n",
    "            (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_W/SCALE/2),\n",
    "            vel.x*(VIEWPORT_W/SCALE/2)/FPS,\n",
    "            vel.y*(VIEWPORT_H/SCALE/2)/FPS,\n",
    "            self.lander.angle,\n",
    "            20.0*self.lander.angularVelocity/FPS,\n",
    "            1.0 if self.legs[0].ground_contact else 0.0,\n",
    "            1.0 if self.legs[1].ground_contact else 0.0\n",
    "            ]\n",
    "        assert len(state)==8\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        shaping = \\\n",
    "            - 200*abs(state[0]) \\\n",
    "            - 200*abs(state[1])\\\n",
    "            - 100*abs(state[2]) \\\n",
    "            - 200*abs(state[3])\\\n",
    "            - 50*abs(state[4]) \\\n",
    "            - 50*abs(state[5])\\\n",
    "            + 30*state[6] + 30*state[7]   # And ten points for legs contact, the idea is if you\n",
    "                                                              # lose contact again after landing, you get negative reward\n",
    "\n",
    "        if self.prev_shaping is not None:\n",
    "            reward = shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        \n",
    "        reward -= m_power*0.3  # less fuel spent is better, about -30 for heurisic landing\n",
    "        reward -= s_power*0.03\n",
    "\n",
    "#         if abs(pos.x)<=5 and abs(pos.y)<=5 and \\\n",
    "#         abs(vel.x)<=1 and abs(vel.y)<=1:\n",
    "#             print(\"within in 5 of target\")\n",
    "#             print(\"velocityes are: \")\n",
    "#             print(vel.x,'---',vel.y)\n",
    "#             reward = reward + 100\n",
    "        \n",
    "        done = False\n",
    "        if self.game_over or abs(state[0]) >= 1.0:\n",
    "            done   = True\n",
    "            reward = -100\n",
    "            \n",
    "        if (not self.lander.awake) or (state[6] == 1 and state[7] == 1\\\n",
    "            and abs(pos.x) <= 5\\\n",
    "            and abs(pos.y) <= 5\\\n",
    "            and abs(vel.x) <=0.5 and abs(vel.y) <=0.1): \n",
    "            print(\"Landed it!\")\n",
    "            done   = True\n",
    "            reward = +200\n",
    "#         if (not self.lander.awake): \n",
    "#             print(\"Landed it!\")\n",
    "#             done   = True\n",
    "#             reward = +200     \n",
    "        return np.array(state), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        from gym.envs.classic_control import rendering\n",
    "        if self.viewer is None:\n",
    "            self.viewer = rendering.Viewer(VIEWPORT_W, VIEWPORT_H)\n",
    "            self.viewer.set_bounds(0, VIEWPORT_W/SCALE, 0, VIEWPORT_H/SCALE)\n",
    "\n",
    "        for obj in self.particles:\n",
    "            obj.ttl -= 0.15\n",
    "            obj.color1 = (max(0.2,0.2+obj.ttl), max(0.2,0.5*obj.ttl), max(0.2,0.5*obj.ttl))\n",
    "            obj.color2 = (max(0.2,0.2+obj.ttl), max(0.2,0.5*obj.ttl), max(0.2,0.5*obj.ttl))\n",
    "\n",
    "        self._clean_particles(False)\n",
    "\n",
    "        for p in self.sky_polys:\n",
    "            self.viewer.draw_polygon(p, color=(0,0,0))\n",
    "\n",
    "        for obj in self.particles + self.drawlist:\n",
    "            for f in obj.fixtures:\n",
    "                trans = f.body.transform\n",
    "                if type(f.shape) is circleShape:\n",
    "                    t = rendering.Transform(translation=trans*f.shape.pos)\n",
    "                    self.viewer.draw_circle(f.shape.radius, 20, color=obj.color1).add_attr(t)\n",
    "                    self.viewer.draw_circle(f.shape.radius, 20, color=obj.color2, filled=False, linewidth=2).add_attr(t)\n",
    "                else:\n",
    "                    path = [trans*v for v in f.shape.vertices]\n",
    "                    self.viewer.draw_polygon(path, color=obj.color1)\n",
    "                    path.append(path[0])\n",
    "                    self.viewer.draw_polyline(path, color=obj.color2, linewidth=2)\n",
    "\n",
    "        for x in [self.helipad_x1, self.helipad_x2]:\n",
    "            flagy1 = self.helipad_y\n",
    "            flagy2 = flagy1 + 50/SCALE\n",
    "            self.viewer.draw_polyline( [(x, flagy1), (x, flagy2)], color=(1,1,1) )\n",
    "            self.viewer.draw_polygon( [(x, flagy2), (x, flagy2-10/SCALE), (x+25/SCALE, flagy2-5/SCALE)], color=(0.8,0.8,0) )\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "class LunarLanderContinuous(LunarLander):\n",
    "    continuous = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(env, s):\n",
    "    # Heuristic for:\n",
    "    # 1. Testing. \n",
    "    # 2. Demonstration rollout.\n",
    "    angle_targ = s[0]*0.5 + s[2]*1.0         # angle should point towards center (s[0] is horizontal coordinate, s[2] hor speed)\n",
    "    if angle_targ >  0.4: angle_targ =  0.4  # more than 0.4 radians (22 degrees) is bad\n",
    "    if angle_targ < -0.4: angle_targ = -0.4\n",
    "    hover_targ = 0.55*np.abs(s[0])           # target y should be proporional to horizontal offset\n",
    "\n",
    "    # PID controller: s[4] angle, s[5] angularSpeed\n",
    "    angle_todo = (angle_targ - s[4])*0.5 - (s[5])*1.0\n",
    "    #print(\"angle_targ=%0.2f, angle_todo=%0.2f\" % (angle_targ, angle_todo))\n",
    "\n",
    "    # PID controller: s[1] vertical coordinate s[3] vertical speed\n",
    "    hover_todo = (hover_targ - s[1])*0.5 - (s[3])*0.5\n",
    "    #print(\"hover_targ=%0.2f, hover_todo=%0.2f\" % (hover_targ, hover_todo))\n",
    "\n",
    "    if s[6] or s[7]: # legs have contact\n",
    "        angle_todo = 0\n",
    "        hover_todo = -(s[3])*0.5  # override to reduce fall speed, that's all we need after contact\n",
    "\n",
    "    if env.continuous:\n",
    "        a = np.array( [hover_todo*20 - 1, -angle_todo*20] )\n",
    "        a = np.clip(a, -1, +1)\n",
    "    else:\n",
    "        a = 0\n",
    "        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05: a = 2\n",
    "        elif angle_todo < -0.05: a = 3\n",
    "        elif angle_todo > +0.05: a = 1\n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnmodel(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(16, activation='sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import subprocess as sp\n",
    "FFMPEG_BIN = \"ffmpeg\" # on Linux ans Mac OS\n",
    "X_train = []\n",
    "y_train = []\n",
    "frames = []\n",
    "command = [ FFMPEG_BIN,\n",
    "        '-y', # (optional) overwrite output file if it exists\n",
    "        '-f', 'rawvideo',\n",
    "        '-vcodec','rawvideo',\n",
    "        '-s', '600x400', # size of one frame\n",
    "        '-pix_fmt', 'rgb24',\n",
    "        '-r', '30', # frames per second\n",
    "        '-i', '-', # The imput comes from a pipe\n",
    "        '-an', # Tells FFMPEG not to expect any audio\n",
    "        '-vcodec', 'mpeg',\n",
    "        'my_output_videofile.mp4' ]\n",
    "\n",
    "import skvideo.io\n",
    "\n",
    "# pipe = sp.Popen(command, stdin=sp.PIPE, stderr=sp.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step  0\n",
      "reward:  11.627838586767439\n",
      "total rewards  0\n",
      "[-0.72584639 -0.16493975  0.37036121 -0.17124475  0.33443683 -1.32066059\n",
      "  0.          1.        ]\n",
      "At step  100\n",
      "reward:  -15.194725294907812\n",
      "total rewards  -398.92145772920935\n",
      "[-0.83246238  0.00886651 -2.79906241 -1.41286479  0.76196969 -4.8111674\n",
      "  0.          1.        ]\n",
      "[-0.2042963  -0.01938707 -1.23735746 -1.46784136  0.54581988 -6.8685557\n",
      "  0.          1.        ]\n",
      "At step  200\n",
      "reward:  -15.045872955643299\n",
      "total rewards  -247.3379338336996\n",
      "[ 0.38145571 -0.07760743  0.54267899 -0.06001727 -0.18633369  0.30782729\n",
      "  1.          1.        ]\n",
      "[ 0.28083544  0.04247342 -0.04690907 -0.01034832 -0.59510529  0.13695459\n",
      "  1.          0.        ]\n",
      "At step  300\n",
      "reward:  -4.186308667823991\n",
      "total rewards  -252.1945014759285\n",
      "[-0.03060751 -0.02158014  0.39642012 -0.83230125 -0.05842352  7.33112335\n",
      "  1.          1.        ]\n",
      "[-0.34957685  0.08388096  0.63681261 -0.30590367  0.22013898 -2.51055892\n",
      "  0.          1.        ]\n",
      "At step  400\n",
      "reward:  -14.643429252836427\n",
      "total rewards  -320.9747246204906\n",
      "[ 0.7675127   0.01890488  1.4578565  -0.32920954 -0.46438619  5.16123009\n",
      "  1.          0.        ]\n",
      "[-8.05619240e-02 -2.85318549e-02 -7.23610620e-02  4.39988020e-08\n",
      "  2.60596484e-04  7.54861276e-08  1.00000000e+00  1.00000000e+00]\n",
      "At step  500\n",
      "reward:  -17.035982643985747\n",
      "total rewards  -277.3722610940621\n",
      "[-0.22738924 -0.02847609 -1.73180962 -1.4529349   0.63238835 -5.51480993\n",
      "  0.          1.        ]\n",
      "[ 2.50111008e-01 -4.33749612e-02  1.25154678e+00 -2.82808489e-01\n",
      " -3.30635965e-01  3.01224408e-09  1.00000000e+00  1.00000000e+00]\n",
      "At step  600\n",
      "reward:  -7.283966791415656\n",
      "total rewards  -359.0591048628446\n",
      "[ 0.25562592 -0.04803116  1.11886724 -1.50053755 -0.38982695 -8.38995171\n",
      "  1.          1.        ]\n",
      "[-0.37601604  0.07079592  0.16131332 -1.10757584 -0.3083882  -7.57453728\n",
      "  0.          1.        ]\n",
      "At step  700\n",
      "reward:  -10.849326253973604\n",
      "total rewards  -269.5560263192549\n",
      "[-0.05232944 -0.01286539 -0.18092777 -0.57312414 -0.2324068   7.00699361\n",
      "  1.          0.        ]\n",
      "At step  800\n",
      "reward:  3.5819080140855704\n",
      "total rewards  -578.3175383651217\n",
      "[ 0.67708607 -0.12591762 -0.07007591 -0.01150091 -0.6738534   0.2000943\n",
      "  1.          0.        ]\n",
      "[-0.59363928 -0.04912221 -1.75244681 -1.49817414  0.36226392 -6.37361272\n",
      "  0.          1.        ]\n",
      "[-0.54989982  0.01858602 -1.81557926 -1.18103207  0.21737066 -7.20994504\n",
      "  1.          1.        ]\n",
      "At step  900\n",
      "reward:  13.196431168665583\n",
      "total rewards  -97.92251892183441\n",
      "[ 1.17120075e-01 -2.85233434e-02  5.00859867e-02  1.14030260e-07\n",
      "  2.04073105e-04 -1.00482369e-06  1.00000000e+00  1.00000000e+00]\n",
      "At step  1000\n",
      "reward:  2.7137191691281615\n",
      "total rewards  -270.49726850303335\n",
      "[ 0.39941597  0.08851991 -0.34852397 -0.3564162  -0.2704834   4.27665997\n",
      "  1.          0.        ]\n",
      "[ 0.05445318 -0.02620179  0.08199054 -1.33383136 -0.03005639  8.46079763\n",
      "  1.          1.        ]\n",
      "At step  1100\n",
      "reward:  -12.331752677758459\n",
      "total rewards  -398.4885660598107\n",
      "[ 0.43414822  0.01773182  0.10918056 -0.32011824 -0.29915646  4.64300887\n",
      "  1.          0.        ]\n",
      "[ 0.08393993 -0.02646479  0.28722612 -1.17935975 -0.00952104  8.01313464\n",
      "  1.          1.        ]\n",
      "At step  1200\n",
      "reward:  -14.606559759670342\n",
      "total rewards  -190.71512382209005\n",
      "[ 4.85853195e-01 -4.40810140e-02  1.64739227e+00 -3.07474693e-01\n",
      " -2.72829354e-01  2.02206725e-07  1.00000000e+00  1.00000000e+00]\n",
      "[ 0.71436062  0.09506241 -0.26651233 -0.3740526  -0.24119867  4.02137947\n",
      "  1.          0.        ]\n",
      "At step  1300\n",
      "reward:  8.616466031800273\n",
      "total rewards  -115.67538828130816\n",
      "[ 0.00139647 -0.02808163 -0.1460721  -0.19357586 -0.00763473  1.03935289\n",
      "  1.          1.        ]\n",
      "At step  1400\n",
      "reward:  23.056150180365208\n",
      "total rewards  -404.9778974256511\n",
      "[-0.18719511  0.00367141  0.61877191 -0.21596094  0.38258216 -1.9358681\n",
      "  0.          1.        ]\n",
      "[ 3.35015583e-01  2.04182307e-03 -1.14088098e-01 -1.67619685e-02\n",
      "  2.17281520e-01  6.44615511e-07  0.00000000e+00  1.00000000e+00]\n",
      "[-0.07778311 -0.02328305 -0.22171388 -1.21886306 -0.09204678  7.64484596\n",
      "  0.          1.        ]\n",
      "At step  1500\n",
      "reward:  1.7172661373263438\n",
      "total rewards  -100.19882161637896\n",
      "[ 1.01904564  0.01867724  2.51848952 -2.93933953 -1.81559837 -0.63525856\n",
      "  1.          0.        ]\n",
      "At step  1600\n",
      "reward:  -7.711955095447939\n",
      "total rewards  -427.72325422791386\n",
      "[-0.45331092 -0.07849118 -1.05892849 -0.15101467  0.6591562  -2.96701527\n",
      "  0.          1.        ]\n",
      "[ 0.41213675  0.04565402 -0.14139247  0.09066792  0.25879383  1.56003491\n",
      "  1.          1.        ]\n",
      "At step  1700\n",
      "reward:  23.52319710458323\n",
      "total rewards  -408.90652504356706\n",
      "[-4.32361174e-01  2.21262614e-03 -8.21303288e-01 -9.29089546e-01\n",
      " -1.20474398e-02 -7.85305913e+00  0.00000000e+00  1.00000000e+00]\n",
      "[-0.28520498 -0.03999683 -1.07219974 -1.1373539   0.12201491  7.44301796\n",
      "  1.          0.        ]\n",
      "At step  1800\n",
      "reward:  -5.951891939057194\n",
      "total rewards  -306.0657084756012\n",
      "[ 0.13209934 -0.01414954 -0.09382509 -1.03544532 -0.27921948  7.89639091\n",
      "  1.          0.        ]\n",
      "[-0.60383623 -0.05072078 -0.87681476 -0.05595768  0.53970188 -1.9883887\n",
      "  0.          1.        ]\n",
      "At step  1900\n",
      "reward:  -7.102871307973862\n",
      "total rewards  -428.92282390882497\n",
      "[ 0.77552452 -0.19807278  0.75806125  0.19445662 -0.35812306  5.3554554\n",
      "  1.          1.        ]\n",
      "[-3.75284100e-01 -4.75969728e-02 -1.31391287e+00 -9.63419676e-02\n",
      "  1.09474152e-01 -5.60329412e-04  1.00000000e+00  1.00000000e+00]\n",
      "At step  2000\n",
      "reward:  -2.5484427672619034\n",
      "total rewards  -255.9371228903579\n",
      "[ 1.00832367  0.02823377  2.43867238 -1.65218427 -0.88530666  0.66572817\n",
      "  0.          0.        ]\n",
      "At step  2100\n",
      "reward:  -3.5406951654646006\n",
      "total rewards  -365.8529175461822\n",
      "[-2.38333178e-01 -2.87307437e-02 -7.18408475e-02 -1.13199853e-04\n",
      "  5.92290517e-03  8.87263178e-04  1.00000000e+00  1.00000000e+00]\n",
      "[ 0.21899853  0.00665084 -0.90820718 -0.42662581 -0.25014573  3.88218943\n",
      "  1.          0.        ]\n",
      "At step  2200\n",
      "reward:  -4.036966248242022\n",
      "total rewards  -396.4829580748999\n",
      "[-0.28700027 -0.04056811 -1.32330275 -0.92081949  0.05556749  6.36998558\n",
      "  1.          0.        ]\n",
      "[-0.50768127 -0.01602744 -0.07221758  0.0514925   1.53471482  0.31759506\n",
      "  0.          1.        ]\n",
      "At step  2300\n",
      "reward:  -8.753638408184052\n",
      "total rewards  -363.0491525881795\n",
      "[ 0.43193388  0.03233848  0.44534262  0.04328416 -1.68693578  0.04510504\n",
      "  1.          0.        ]\n",
      "[ 4.10628891e-01  4.26858107e-03  1.66923443e+00 -5.98332882e-01\n",
      " -7.94455290e-01  4.67290338e+00  1.00000000e+00  0.00000000e+00]\n",
      "At step  2400\n",
      "reward:  0.7845719453631445\n",
      "total rewards  -242.56140598358525\n",
      "[ 0.14321728 -0.02089974  0.29523756 -1.10078165 -0.13409211  7.50899824\n",
      "  1.          1.        ]\n",
      "[ 0.2595417  -0.01412796  1.601717   -0.37178254 -0.83599842  2.41510312\n",
      "  1.          0.        ]\n",
      "At step  2500\n",
      "reward:  2.550057751008632\n",
      "total rewards  -344.5441944039987\n",
      "[-0.2463819  -0.03356218 -0.79859877 -1.35422516  0.11327038  8.99616687\n",
      "  1.          0.        ]\n",
      "[ 0.60098076  0.03455263  0.84916321  0.49484852 -1.68003893 -3.38701566\n",
      "  1.          0.        ]\n",
      "At step  2600\n",
      "reward:  0.8117908797024544\n",
      "total rewards  -293.134224634326\n",
      "[ 0.18757238 -0.00991639 -0.98755153 -0.88339186  0.07470447  6.39622116\n",
      "  1.          1.        ]\n",
      "[-0.68446236 -0.01483499 -0.97243555  0.10448786  0.78910613  2.73007647\n",
      "  0.          1.        ]\n",
      "At step  2700\n",
      "reward:  0.9279688300874624\n",
      "total rewards  -425.5783366732611\n",
      "[-0.66740301 -0.14049868 -2.43092537 -0.52452273  0.85764599 -2.63898945\n",
      "  0.          1.        ]\n",
      "[ 0.27838821  0.01093584 -0.19742856 -0.09180546 -0.56502777  1.50110626\n",
      "  1.          0.        ]\n",
      "At step  2800\n",
      "reward:  -11.822259467509127\n",
      "total rewards  -277.43682883535126\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    env = LunarLanderContinuous()\n",
    "    prev_s = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    a = np.array([0.0,0.0])\n",
    "    modelTrained = False\n",
    "#     model = linear_model.SGDRegressor()\n",
    "    model = nnmodel(10)\n",
    "    tr = 0 \n",
    "    prev_r = 0\n",
    "    training_thr = 3000\n",
    "    total_itrs = 50000\n",
    "    successful_steps = []\n",
    "    while steps <= total_itrs:\n",
    "        \n",
    "#         a = heuristic(env, s)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "        X_train.append(list(prev_s)+list(a))\n",
    "        y_train.append(r)\n",
    "        if steps > training_thr and steps %100 ==0:\n",
    "            # re-train a model \n",
    "            print(\"training model model\")\n",
    "            modelTrained = True\n",
    "#             model.fit(np.array(X_train),np.array(y_train).reshape(len(y_train),1))\n",
    "            model.fit(np.array(X_train),np.array(y_train).reshape(len(y_train),1), epochs = 10, batch_size=20)\n",
    "\n",
    "        if modelTrained:\n",
    "            maxr = -1000\n",
    "            maxa = None\n",
    "            for i in range(100):\n",
    "                a1 = np.random.randint(-1000,1000)/1000\n",
    "                a2 = np.random.randint(-1000,1000)/1000\n",
    "                testa = [a1,a2]\n",
    "                r_pred = model.predict(np.array(list(new_s)+list(testa)).reshape(1,len(list(new_s)+list(testa))))\n",
    "                if r_pred > maxr:\n",
    "                    maxr = r_pred\n",
    "                    maxa = testa\n",
    "            a = np.array(maxa)\n",
    "            \n",
    "        else:\n",
    "            a = np.array([np.random.randint(-1000,1000)/1000,\\\n",
    "                 np.random.randint(-1000,1000)/1000])\n",
    "    \n",
    "        if steps %100 == 0:\n",
    "            print(\"At step \", steps)\n",
    "            print(\"reward: \", r)\n",
    "            print(\"total rewards \", tr)\n",
    "        prev_s = new_s\n",
    "        prev_r = r\n",
    "        if (steps >= training_thr and tr < -500) or done:\n",
    "            print(prev_s)\n",
    "            if done and prev_r >= 200:\n",
    "                successful_steps.append(steps)\n",
    "                print(\"Successful Landing!!! \",steps)\n",
    "                print(\"Total successes are: \", len(successful_steps))\n",
    "            env.reset()\n",
    "            tr = 0 \n",
    "        # train the random forest every 1000 iterations:\n",
    "        tr = tr + r\n",
    "        \n",
    "        steps += 1\n",
    "        env.render(mode='human')\n",
    "#         frame = env.render(mode='rgb_array')\n",
    "#         skvideo.io.vwrite(\"outputvideo.mp4\", frame)\n",
    "#         del frame\n",
    "#         env.render()\n",
    "#         total_reward += r\n",
    "#         if steps % 5 == 0 or done:\n",
    "#             print([\"{:+0.2f}\".format(x) for x in s])\n",
    "#             print(\"step {} total_reward {:+0.2f}\".format(steps, total_reward))\n",
    "#             print(\"info: \", info)\n",
    "#         steps += 1\n",
    "#         if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(successful_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train)\n",
    "plt.ylim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(successful_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit(np.array(X_train),np.array(y_train).reshape(len(y_train),1), epochs = 1000, batch_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    env = LunarLanderContinuous()\n",
    "    prev_s = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    a = np.array([0.0,0.0])\n",
    "    modelTrained = True\n",
    "    tr = 0 \n",
    "    prev_r = 0\n",
    "    training_thr = 3000\n",
    "    total_itrs = 50000\n",
    "    successful_steps = []\n",
    "    while steps <= total_itrs:\n",
    "        \n",
    "#         a = heuristic(env, s)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        if modelTrained:\n",
    "            maxr = -1000\n",
    "            maxa = None\n",
    "            for i in range(100):\n",
    "                a1 = np.random.randint(-1000,1000)/1000\n",
    "                a2 = np.random.randint(-1000,1000)/1000\n",
    "                testa = [a1,a2]\n",
    "                r_pred = model.predict(np.array(list(new_s)+list(testa)).reshape(1,len(list(new_s)+list(testa))))\n",
    "                if r_pred > maxr:\n",
    "                    maxr = r_pred\n",
    "                    maxa = testa\n",
    "            a = np.array(maxa)\n",
    "            \n",
    "        else:\n",
    "            a = np.array([np.random.randint(-1000,1000)/1000,\\\n",
    "                 np.random.randint(-1000,1000)/1000])\n",
    "    \n",
    "        if steps %100 == 0:\n",
    "            print(\"At step \", steps)\n",
    "            print(\"reward: \", r)\n",
    "            print(\"total rewards \", tr)\n",
    "        prev_s = new_s\n",
    "        prev_r = r\n",
    "        if (steps >= training_thr and tr < -500) or done:\n",
    "            print(prev_s)\n",
    "            if done and prev_r >= 200:\n",
    "                successful_steps.append(steps)\n",
    "                print(\"Successful Landing!!! \",steps)\n",
    "                print(\"Total successes are: \", len(successful_steps))\n",
    "            env.reset()\n",
    "            tr = 0 \n",
    "        # train the random forest every 1000 iterations:\n",
    "        tr = tr + r\n",
    "        \n",
    "        steps += 1\n",
    "        env.render(mode='human')\n",
    "#         frame = env.render(mode='rgb_array')\n",
    "#         skvideo.io.vwrite(\"outputvideo.mp4\", frame)\n",
    "#         del frame\n",
    "#         env.render()\n",
    "#         total_reward += r\n",
    "#         if steps % 5 == 0 or done:\n",
    "#             print([\"{:+0.2f}\".format(x) for x in s])\n",
    "#             print(\"step {} total_reward {:+0.2f}\".format(steps, total_reward))\n",
    "#             print(\"info: \", info)\n",
    "#         steps += 1\n",
    "#         if done: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
