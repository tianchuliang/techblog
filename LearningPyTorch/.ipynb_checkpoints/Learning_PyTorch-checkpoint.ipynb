{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Neural Nets with Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any NN, we need to define \n",
    "1. N, batch size (how many data points we batch together to feed into the network for each training iteration)\n",
    "2. D_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, w2):\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    return h,h_relu,y_pred \n",
    "\n",
    "def backprop(y_pred,y,w2,x):\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    return grad_w1,grad_w2\n",
    "\n",
    "def update_weights(learning_rate, wts, grads):\n",
    "    wts = [wts[i] - learning_rate * grads[i] for i in range(len(grads))]\n",
    "    return wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25821717.79594508\n",
      "10 2990815.177038224\n",
      "20 227958.96683542518\n",
      "30 72527.36720239455\n",
      "40 27961.29320001861\n",
      "50 11880.112144158691\n",
      "60 5380.362705199739\n",
      "70 2545.897482907582\n",
      "80 1245.3431924239312\n",
      "90 624.9581993955965\n",
      "100 320.2348613321201\n",
      "110 167.33291976395515\n",
      "120 88.8152181174022\n",
      "130 47.7252230712143\n",
      "140 25.920790522282537\n",
      "150 14.209109787031826\n",
      "160 7.8520446822132195\n",
      "170 4.369968528844953\n",
      "180 2.447307484721206\n",
      "190 1.3781972657884225\n",
      "200 0.7799313257244906\n",
      "210 0.44332902648636324\n",
      "220 0.2529947194111715\n",
      "230 0.14488079533582207\n",
      "240 0.08323144775276493\n",
      "250 0.04795127498920168\n",
      "260 0.02769625436983872\n",
      "270 0.016033158612897818\n",
      "280 0.009300558287029442\n",
      "290 0.005405152812189648\n",
      "300 0.0031464521613622314\n",
      "310 0.001834383675435549\n",
      "320 0.0010708575092865313\n",
      "330 0.0006259054870571197\n",
      "340 0.00036623321506733025\n",
      "350 0.00021451211607372085\n",
      "360 0.0001257545364392423\n",
      "370 7.378339550583401e-05\n",
      "380 4.332232927972118e-05\n",
      "390 2.5453728709243733e-05\n",
      "400 1.4964762563234315e-05\n",
      "410 8.802640699564468e-06\n",
      "420 5.180838909080549e-06\n",
      "430 3.0505341145513233e-06\n",
      "440 1.7969062266929062e-06\n",
      "450 1.058922598774865e-06\n",
      "460 6.242382866360144e-07\n",
      "470 3.681106370226414e-07\n",
      "480 2.171460589750339e-07\n",
      "490 1.281238102996225e-07\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h,h_relu,y_pred = forward(x, w1, w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    losses.append(loss)\n",
    "    if t%10 == 0:\n",
    "        print(t,loss)\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_w1, grad_w2 = backprop(y_pred, y, w2, x)\n",
    "\n",
    "    # Update weights\n",
    "    newwts = update_weights(learning_rate, [w1,w2],[grad_w1,grad_w2])\n",
    "    w1 = newwts[0]\n",
    "    w2 = newwts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x108151a20>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107500860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Neural Nets with PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, w2):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    return h,h_relu,y_pred \n",
    "\n",
    "def backprop(y_pred,y,w2,x):\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    return grad_w1,grad_w2\n",
    "\n",
    "def update_weights(learning_rate, wts, grads):\n",
    "    wts = [wts[i] - learning_rate * grads[i] for i in range(len(grads))]\n",
    "    return wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.6598e+07)\n",
      "10 tensor(1.00000e+06 *\n",
      "       2.8419)\n",
      "20 tensor(1.00000e+05 *\n",
      "       2.2178)\n",
      "30 tensor(67892.1172)\n",
      "40 tensor(26809.6094)\n",
      "50 tensor(11988.1670)\n",
      "60 tensor(5802.9707)\n",
      "70 tensor(2969.6003)\n",
      "80 tensor(1582.7921)\n",
      "90 tensor(868.9327)\n",
      "100 tensor(488.3370)\n",
      "110 tensor(279.6025)\n",
      "120 tensor(162.5007)\n",
      "130 tensor(95.5657)\n",
      "140 tensor(56.7578)\n",
      "150 tensor(33.9919)\n",
      "160 tensor(20.5045)\n",
      "170 tensor(12.4466)\n",
      "180 tensor(7.5963)\n",
      "190 tensor(4.6592)\n",
      "200 tensor(2.8702)\n",
      "210 tensor(1.7752)\n",
      "220 tensor(1.1018)\n",
      "230 tensor(0.6862)\n",
      "240 tensor(0.4287)\n",
      "250 tensor(0.2685)\n",
      "260 tensor(0.1686)\n",
      "270 tensor(0.1062)\n",
      "280 tensor(1.00000e-02 *\n",
      "       6.7001)\n",
      "290 tensor(1.00000e-02 *\n",
      "       4.2414)\n",
      "300 tensor(1.00000e-02 *\n",
      "       2.6917)\n",
      "310 tensor(1.00000e-02 *\n",
      "       1.7159)\n",
      "320 tensor(1.00000e-02 *\n",
      "       1.1007)\n",
      "330 tensor(1.00000e-03 *\n",
      "       7.1183)\n",
      "340 tensor(1.00000e-03 *\n",
      "       4.6564)\n",
      "350 tensor(1.00000e-03 *\n",
      "       3.0992)\n",
      "360 tensor(1.00000e-03 *\n",
      "       2.0996)\n",
      "370 tensor(1.00000e-03 *\n",
      "       1.4569)\n",
      "380 tensor(1.00000e-03 *\n",
      "       1.0351)\n",
      "390 tensor(1.00000e-04 *\n",
      "       7.5332)\n",
      "400 tensor(1.00000e-04 *\n",
      "       5.6060)\n",
      "410 tensor(1.00000e-04 *\n",
      "       4.2620)\n",
      "420 tensor(1.00000e-04 *\n",
      "       3.2967)\n",
      "430 tensor(1.00000e-04 *\n",
      "       2.6052)\n",
      "440 tensor(1.00000e-04 *\n",
      "       2.0823)\n",
      "450 tensor(1.00000e-04 *\n",
      "       1.7000)\n",
      "460 tensor(1.00000e-04 *\n",
      "       1.4068)\n",
      "470 tensor(1.00000e-04 *\n",
      "       1.1741)\n",
      "480 tensor(1.00000e-05 *\n",
      "       9.9372)\n",
      "490 tensor(1.00000e-05 *\n",
      "       8.5533)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h,h_relu,y_pred = forward(x, w1, w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    losses.append(loss)\n",
    "    if t%10 == 0:\n",
    "        print(t,loss)\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_w1, grad_w2 = backprop(y_pred, y, w2, x)\n",
    "\n",
    "    # Update weights\n",
    "    newwts = update_weights(learning_rate, [w1,w2],[grad_w1,grad_w2])\n",
    "    w1 = newwts[0]\n",
    "    w2 = newwts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10acb5da0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10816e128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "losses = []\n",
    "for t in range(500):\n",
    "    # Forward pass: we chain operations on x by using the . notation \n",
    "    # consecutively, kind of like Scala\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors..\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    losses.append(loss)\n",
    "    if t%10 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This is dependent on setting \n",
    "    # grad_required to True in the weights variables. This is really cool; this\n",
    "    # means PyTorch tensors inherently can be configureed to be \"differentiable\"\n",
    "    # when needed. \n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
